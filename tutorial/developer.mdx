---
title: "Developer"
description: "Detailed tutorial for benchmark developers."
---

## Build your benchmark with BenchFlow

We provide benchmark developers with two interfaces to interact with intelligence: **BenchClient** and **BaseBench**.

<CardGroup cols="2">
  <Card title="BenchClient" icon="sparkles">
    BenchClient make your benchmark as a client, which enables seamless interaction with the intelligence through HTTP. It should be embedded into the evaluation entrance.
  </Card>

  <Card title="BaseBench" icon="sparkles">
    BaseBench is an interface for running, managing, and displaying benchmark results. All benchmark outputs are unified, enabling standardized visualization on BenchFlow.
  </Card>
</CardGroup>

## Install benchflow sdk

<CodeGroup>
  ```bash uv
  uv add benchflow
  ```

  ```bash pip
  pip install benchflow
  ```
</CodeGroup>

## Make your benchmark a client

We go through the entire benchmark onboarding process by integrating **MMLU-Pro** into BenchFlow as an example.

### Import BenchClient

```python
from benchflow import BenchClient
```

### Extend BenchClient

You need to implement two methods.&#x20;
`parse_input` defines the structure of data provided by the benchmark, and it returns a dictionary.
`parse_response`is used to parse the raw response from the agent into a structured dictionary.

```python
class MMLUClient(BenchClient):
    def __init__(self, intelligence_url):
        super().__init__(intelligence_url)
```

<Info>
  The `intelligence_url` is an address used for communicating with the agent. Your evaluation script should provide an argument to accept this URL. We will explain the detail in subsequent steps.
</Info>

```python
def prepare_input(self, env: Dict[str, Any]) -> Dict[str, Any]:
        single_question = env["each"]
        cot_examples_dict = env["input_text"]
        category = single_question["category"]
        cot_examples = cot_examples_dict[category]
        question = single_question["question"]
        options = single_question["options"]
        prompt = "The following are multiple choice questions (with answers) about {}. Think step by step and then output the answer in the format of \"The answer is (X)\" at the end.\n\n".format(category)
        for each in cot_examples:
            prompt += format_example(each["question"], each["options"], each["cot_content"])
        input_text = format_example(question, options)
        return {"prompt": prompt, "input_text": input_text, "entry": single_question, "cot_examples_dict": cot_examples_dict}
```

The basic benchmark provided by mmlu-pro consists of four dictionaries, which are as follows:

\{
`  "prompt"`: prompt,
`  "input_text"`: input\_text,
`  "entry"`: single\_question,
`  "cot_examples_dict"`: cot\_examples\_dict
`}`

<Note>
  Benchmark developers should clearly document in the README (model card) the keys and their meanings of the input data provided. This is essential for intelligence developers to benchmark.
</Note>

```python
    def parse_response(self, raw_response: str) -> Dict[str, Any]:
        # extract_answer() is provided by MMLU-Pro developer
        pred = extract_answer(raw_response) 
        return {"action": pred, "response": raw_response}
```

### Use get\_response to receive response from intelligence