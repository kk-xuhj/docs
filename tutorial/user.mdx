---
title: "User"
description: "Detail tutorial for benchmark users."
---

## Benchmark your intelligence

**Intelligence** is any AI-related product (e.g.LLM models, AI agents ...).&#x20;

Benchflow use the api provided by your intelligence to help you rapidly build a benchmark pipeline without any benchmark setup.

## Install benchflow sdk

<CodeGroup>
  ```bash uv
  uv add benchflow
  ```

  ```bash pip
  pip install benchflow
  ```
</CodeGroup>

## Implement the interface for api

<AccordionGroup>
  <Accordion title="Import the interface for api" defaultOpen={false}>
    You need to implement the **BaseAgen**t interface provided by benchflow.

    ```python
    from benchflow import BaseAgent
    ```

    <Note>
      Although this interface is named BaseAgent, **you don’t need to design an AI agent**; simply implementing the call\_api method is sufficient. We call it an “agent” because it serves as an agent to invoke your API.
    </Note>
  </Accordion>

  <Accordion title="Check the benchmark card" defaultOpen={false}>
    Go to the Benchmark Hub and read **model card** about the benchmarks you want to test on, especially the **task\_step\_input** provided by the benchmark developer.

    <Check>
      The **`task_step_input`** is a dictionary provided as input to the **`call_api`** method that contains all the benchmark dataset information. You will need to use it to test your intelligence.
    </Check>
  </Accordion>

  <Accordion title="Implement your call_api function" defaultOpen={false}>
    Here is a basic example about testing the OpenAI on a Q\&A benchmark. Suppose the model card specifies that the format of `task_step_inputs` is `{"question": "question text"}`.

    ```python openai_caller.py
    from openai import OpenAI
    from benchflow import BaseAgent

    class YourCaller(BaseAgent):
      def call_api(self, task_step_inputs: Dict[str, Any]) -> str:
          messages = [
            {"role": "user", "content": task_step_inputs['question']}
          ]
          client = OpenAI(
             api_key=os.getenv("OPENAI_API_KEY"),
          )
          response = client.chat.completions.create(
             messages=messages,
             model="gpt-4o",
             temperature=0.9,
          )
          content = response.choices[0].message.content
          return content
    ```

    <Note>
      Please implement this interface in a separate file. In the future, we will support more flexible implementation approaches.
    </Note>
  </Accordion>
</AccordionGroup>

## Run the benchmark

<AccordionGroup>
  <Accordion title="Create the environment for calling your api" defaultOpen={false}>
    We require you to provide a Python-style **`requirements.txt`** file that lists all dependencies needed to call your API. For instance, in the **`openai_caller.py`** , your `requirements.txt` should include the following dependencies:

    ```
    openai
    benchflow
    ```
  </Accordion>

  <Accordion title="Get your BenchFlow token" defaultOpen={false}>
    Kickstart your free BenchFlow trial on <a href="https://benchflow.ai" target="_blank" rel="noopener noreferrer">BenchFlow.ai</a> to unlock benchmarking insights.
  </Accordion>

  <Accordion title="Benchmark your intelligence" defaultOpen={false}>
    Load your benchmark from benchmark hub.

    ```python
    from benchflow import load_benchmark
    bench = load_benchmark("benchflow/webarena", bf_token=os.getenv(BF_TOKEN))
    ```

    <Note>
      The naming format for benchmarks is **`organization_name/benchmark_name`**
    </Note>

    Import your api caller (agent).

    ```python
    from api_caller import YourCaller

    agent = YourCaller()
    ```

    Start the benchmark tasks.

    ```python
    # Refer to the fileds description below for more details
    run_ids = bench.run(
            task_ids=[0],
            agents=agent,
            api={
                "provider": "openai", 
                "model": "gpt-4o-mini", 
                "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY")
            },
            requirements_txt="webarena_requirements.txt",
            args={}
        )
    ```

    <Accordion title="Fields Description" defaultOpen={false} icon="info" iconType="solid">
      * **task\_ids:**

        A list that specifies which task(s) to run. If you leave this list empty, the system defaults to running the full benchmark. For the precise format, please refer to the model card.

      * **agents:**

        An instance of your agent that implements the `call_api` method.&#x20;

      * **api:**

        A dictionary containing the API configuration details. **This should includes the provider name, model, and any necessary API keys.**&#x20;

      * **requirements\_txt:**

        A file path to your dependencies file formatted like a standard `requirements.txt`. This file should list all the Python dependencies needed for your API calls.

      * **args:**

        A dictionary for any **required** and **optional** arguments required by your benchmark. Please refer to the model card on the benchmark hub.
    </Accordion>
  </Accordion>

  <Accordion title="Get your results" defaultOpen={false}>
    You can get your results from our sdk:

    ```
     results = bench.get_results(run_ids)
    ```

    or download the results from [BenchFlow.ai dashborad](https://staging.benchflow.ai/dashboard/jobs).
  </Accordion>
</AccordionGroup>

## Complex Examples

<CardGroup cols={3}>
  <Card title="Webarena" icon="sparkles" />

  <Card title="Rarebench" icon="sparkles" />

  <Card title="Webcanvas" icon="sparkles" />
</CardGroup>

##